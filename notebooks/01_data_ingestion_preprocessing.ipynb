{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75406c08",
   "metadata": {},
   "source": [
    "# Import independecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b736e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/01_data_ingestion_preprocessing.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from telethon import TelegramClient, events\n",
    "from telethon.tl.types import MessageMediaPhoto, DocumentAttributeFilename\n",
    "from telethon.tl.functions.channels import GetParticipantsRequest\n",
    "from telethon.tl.types import ChannelParticipantsSearch\n",
    "from tqdm.notebook import tqdm\n",
    "import re \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda532a",
   "metadata": {},
   "source": [
    "# Project root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56614a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM4\\amharic-ecommerce-data-extractor' to sys.path for module imports.\n",
      "Project configuration loaded. Data directories created/verified.\n",
      "Telegram Channels configured: ['@nevacomputer', '@marakibrand', '@Fashiontera', '@Shewabrand', '@ethio_brand_collection']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Project Setup: Ensure src module is discoverable ---\n",
    "# This block is crucial for importing from src.config\n",
    "def find_project_root(current_path):\n",
    "    path = current_path\n",
    "    while path != os.path.dirname(path):\n",
    "        if (os.path.isdir(os.path.join(path, 'src')) and\n",
    "            os.path.isdir(os.path.join(path, 'data')) and\n",
    "            os.path.isdir(os.path.join(path, 'notebooks'))):\n",
    "            return path\n",
    "        path = os.path.dirname(path)\n",
    "    return current_path\n",
    "\n",
    "current_working_dir = os.getcwd()\n",
    "project_root = find_project_root(current_working_dir)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Added '{project_root}' to sys.path for module imports.\")\n",
    "else:\n",
    "    print(f\"'{project_root}' already in sys.path.\")\n",
    "\n",
    "# Import configuration variables\n",
    "from src.config import (\n",
    "    TELEGRAM_API_ID, TELEGRAM_API_HASH, TELEGRAM_CHANNELS,\n",
    "    RAW_MESSAGES_JSON, CLEAN_MESSAGES_CSV, IMAGE_DOWNLOAD_DIR,\n",
    "    RAW_DATA_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17927b57",
   "metadata": {},
   "source": [
    "# Data Ingestiona and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6edfdf4",
   "metadata": {},
   "source": [
    "*Telegram API client setup*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f020f2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-Data Ingestion and Preprocessing ---\n",
      "\n",
      "--- Initiating Telegram Channel Scraping ---\n",
      "\n",
      "--- Connecting to Telegram... ---\n",
      "Connected and authenticated with Telegram successfully.\n",
      "\n",
      "Processing channel: @nevacomputer\n",
      "Resolved channel: NEVA COMPUTERÂ® (ID: 1195361398)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155824057b5847cdb8946e0429ea615d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping NEVA COMPUTERÂ®...:   0%|          | 0/500 [00:00<?, ?msg/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing channel: @marakibrand\n",
      "Resolved channel: áˆ›áˆ«áŠª áƒªÐ¯ï¾‘Å‹ã®â„¢ (ID: 1320403852)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6feb13a469c24a338d46d3cc9205ebc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping áˆ›áˆ«áŠª áƒªÐ¯ï¾‘Å‹ã®â„¢...:   0%|          | 0/500 [00:00<?, ?msg/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing channel: @Fashiontera\n",
      "Resolved channel: Fashion tera (ID: 1175527648)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18fb7455d764ea59c8efe0607435a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping Fashion tera...:   0%|          | 0/500 [00:00<?, ?msg/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing channel: @Shewabrand\n",
      "Resolved channel: Shewa Brand (ID: 1237900032)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf2b4a8fffa4d269c6d8906213d76d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping Shewa Brand...:   0%|          | 0/500 [00:00<?, ?msg/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing channel: @ethio_brand_collection\n",
      "Resolved channel: EthioBrandÂ® (ID: 1149977975)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bed09e026e4332b70c61f52412799b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping EthioBrandÂ®...:   0%|          | 0/500 [00:00<?, ?msg/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Raw Message Collection Summary ---\n",
      "Total raw messages collected: 2500\n",
      "Raw DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2500 entries, 0 to 2499\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   message_id       2500 non-null   int64  \n",
      " 1   channel_id       2500 non-null   int64  \n",
      " 2   channel_name     2500 non-null   object \n",
      " 3   sender_id        2500 non-null   int64  \n",
      " 4   date             2500 non-null   object \n",
      " 5   text_content     2498 non-null   object \n",
      " 6   has_media        2500 non-null   bool   \n",
      " 7   media_type       2482 non-null   object \n",
      " 8   media_file_name  2482 non-null   object \n",
      " 9   media_file_path  2482 non-null   object \n",
      " 10  views            2498 non-null   float64\n",
      "dtypes: bool(1), float64(1), int64(3), object(6)\n",
      "memory usage: 197.9+ KB\n",
      "\n",
      "First 5 rows of Raw Messages:\n",
      "   message_id  channel_id    channel_name      sender_id  \\\n",
      "0        8779  1195361398  NEVA COMPUTERÂ® -1001195361398   \n",
      "1        8778  1195361398  NEVA COMPUTERÂ® -1001195361398   \n",
      "2        8777  1195361398  NEVA COMPUTERÂ® -1001195361398   \n",
      "3        8776  1195361398  NEVA COMPUTERÂ® -1001195361398   \n",
      "4        8775  1195361398  NEVA COMPUTERÂ® -1001195361398   \n",
      "\n",
      "                        date  \\\n",
      "0  2025-06-21T06:32:27+00:00   \n",
      "1  2025-06-21T06:32:27+00:00   \n",
      "2  2025-06-21T06:32:27+00:00   \n",
      "3  2025-06-21T06:32:27+00:00   \n",
      "4  2025-06-11T13:56:52+00:00   \n",
      "\n",
      "                                        text_content  has_media media_type  \\\n",
      "0  ðŸ’» Dell Precision Workstation\\n\\nA powerful per...       True      photo   \n",
      "1                                                          True      photo   \n",
      "2                                                          True      photo   \n",
      "3                                                          True      photo   \n",
      "4                                                          True      photo   \n",
      "\n",
      "                         media_file_name  \\\n",
      "0  channel_1195361398_msg_8779_photo.jpg   \n",
      "1  channel_1195361398_msg_8778_photo.jpg   \n",
      "2  channel_1195361398_msg_8777_photo.jpg   \n",
      "3  channel_1195361398_msg_8776_photo.jpg   \n",
      "4  channel_1195361398_msg_8775_photo.jpg   \n",
      "\n",
      "                                     media_file_path   views  \n",
      "0  c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM4\\amh...   707.0  \n",
      "1  c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM4\\amh...   707.0  \n",
      "2  c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM4\\amh...   707.0  \n",
      "3  c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM4\\amh...   613.0  \n",
      "4  c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM4\\amh...  1694.0  \n",
      "Saving raw messages to: c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM4\\amharic-ecommerce-data-extractor\\data\\raw\\raw_telegram_messages.json\n",
      "Raw messages saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n-Data Ingestion and Preprocessing ---\")\n",
    "\n",
    "# --- Step 1: Telegram API Client Setup ---\n",
    "# A session name is used to save login information so you don't have to log in every time\n",
    "session_name = 'telegram_scraper_session'\n",
    "client = TelegramClient(session_name, TELEGRAM_API_ID, TELEGRAM_API_HASH)\n",
    "\n",
    "async def connect_and_authenticate():\n",
    "    \"\"\"Connects to Telegram and authenticates the user.\"\"\"\n",
    "    print(\"\\n--- Connecting to Telegram... ---\")\n",
    "    await client.start()\n",
    "    if not await client.is_user_authorized():\n",
    "        print(\"Please log in to your Telegram account.\")\n",
    "        # This will prompt for phone number and code in the console if not already logged in\n",
    "        await client.send_code_request(phone=input('Enter phone number: '))\n",
    "        await client.sign_in(phone=input('Enter phone number: '), code=input('Enter code: '))\n",
    "    print(\"Connected and authenticated with Telegram successfully.\")\n",
    "\n",
    "# --- Step 2: Data Ingestion System (Scraping Messages) ---\n",
    "async def scrape_telegram_channels():\n",
    "    \"\"\"Scrapes messages (text and media) from configured Telegram channels.\"\"\"\n",
    "    all_messages_data = []\n",
    "\n",
    "    print(\"\\n--- Initiating Telegram Channel Scraping ---\")\n",
    "    \n",
    "    await connect_and_authenticate()\n",
    "\n",
    "    for channel_id_or_username in TELEGRAM_CHANNELS:\n",
    "        print(f\"\\nProcessing channel: {channel_id_or_username}\")\n",
    "        try:\n",
    "            entity = await client.get_entity(channel_id_or_username)\n",
    "            print(f\"Resolved channel: {entity.title} (ID: {entity.id})\")\n",
    "\n",
    "            # Fetch messages into a list first, then iterate with tqdm\n",
    "            # This resolves the 'async for requires an object with __aiter__' error\n",
    "            # as tqdm.notebook.tqdm expects a synchronous iterable.\n",
    "            # You can adjust the limit for number of messages to fetch per channel.\n",
    "            # For very large channels, consider fetching in smaller chunks or over a specific date range.\n",
    "            messages_fetched = await client.get_messages(entity, limit=500) \n",
    "\n",
    "            pbar_desc = f\"Scraping {entity.title[:20]}...\" # Truncate for display\n",
    "            for message in tqdm(messages_fetched, desc=pbar_desc, unit=\"msg\"):\n",
    "                msg_data = {\n",
    "                    'message_id': message.id,\n",
    "                    'channel_id': entity.id,\n",
    "                    'channel_name': entity.title,\n",
    "                    'sender_id': message.sender_id,\n",
    "                    'date': message.date.isoformat(),\n",
    "                    'text_content': message.message,\n",
    "                    'has_media': False,\n",
    "                    'media_type': None,\n",
    "                    'media_file_name': None,\n",
    "                    'media_file_path': None,\n",
    "                    'views': message.views # Engagement metric\n",
    "                }\n",
    "\n",
    "                if message.media:\n",
    "                    msg_data['has_media'] = True\n",
    "                    if isinstance(message.media, MessageMediaPhoto):\n",
    "                        msg_data['media_type'] = 'photo'\n",
    "                        # Download photo\n",
    "                        photo_filename = f\"channel_{entity.id}_msg_{message.id}_photo.jpg\"\n",
    "                        photo_filepath = os.path.join(IMAGE_DOWNLOAD_DIR, photo_filename)\n",
    "                        try:\n",
    "                            await client.download_media(message.media, file=photo_filepath)\n",
    "                            msg_data['media_file_name'] = photo_filename\n",
    "                            msg_data['media_file_path'] = photo_filepath\n",
    "                        except Exception as e:\n",
    "                            print(f\"  Warning: Could not download photo for message {message.id} in {entity.title}: {e}\")\n",
    "                            msg_data['media_file_name'] = 'download_failed'\n",
    "                    elif message.document: # Handle other documents like files\n",
    "                        for attr in message.document.attributes:\n",
    "                            if isinstance(attr, DocumentAttributeFilename):\n",
    "                                msg_data['media_type'] = 'document'\n",
    "                                doc_filename = f\"channel_{entity.id}_msg_{message.id}_{attr.file_name}\"\n",
    "                                doc_filepath = os.path.join(IMAGE_DOWNLOAD_DIR, doc_filename)\n",
    "                                try:\n",
    "                                    await client.download_media(message.media, file=doc_filepath)\n",
    "                                    msg_data['media_file_name'] = doc_filename\n",
    "                                    msg_data['media_file_path'] = doc_filepath\n",
    "                                except Exception as e:\n",
    "                                    print(f\"  Warning: Could not download document for message {message.id} in {entity.title}: {e}\")\n",
    "                                    msg_data['media_file_name'] = 'download_failed'\n",
    "                                break\n",
    "                all_messages_data.append(msg_data)\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"ERROR: Could not find channel/entity '{channel_id_or_username}'. Please check its exact username or ID. Error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: An unexpected error occurred while processing '{channel_id_or_username}': {e}\")\n",
    "    \n",
    "    await client.disconnect()\n",
    "    return all_messages_data\n",
    "\n",
    "# Run the scraping process\n",
    "# Use asyncio.run() to run the async function\n",
    "messages_list = await scrape_telegram_channels() # In a Jupyter notebook, 'await' works directly at top-level\n",
    "\n",
    "if not messages_list:\n",
    "    print(\"\\nCRITICAL WARNING: No messages were collected from any channel. Please check channel configurations and API keys.\")\n",
    "    raw_df = pd.DataFrame()\n",
    "else:\n",
    "    print(f\"\\n--- Raw Message Collection Summary ---\")\n",
    "    raw_df = pd.DataFrame(messages_list)\n",
    "    print(f\"Total raw messages collected: {len(raw_df)}\")\n",
    "    print(\"Raw DataFrame Info:\")\n",
    "    raw_df.info()\n",
    "    print(\"\\nFirst 5 rows of Raw Messages:\")\n",
    "    print(raw_df.head())\n",
    "    print(f\"Saving raw messages to: {RAW_MESSAGES_JSON}\")\n",
    "    # Save as JSON as it's easier to store complex dicts, especially if media info gets complicated\n",
    "    with open(RAW_MESSAGES_JSON, 'w', encoding='utf-8') as f:\n",
    "        json.dump(messages_list, f, ensure_ascii=False, indent=4)\n",
    "    print(\"Raw messages saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b077306",
   "metadata": {},
   "source": [
    "# Preprocessing and Structuring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd162f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Initiating Data Preprocessing and Structuring ---\n",
      "\n",
      "Applying Amharic-specific text preprocessing...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mApplying Amharic-specific text preprocessing...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m clean_df[\u001b[33m'\u001b[39m\u001b[33mprocessed_text\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mclean_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext_content\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_amharic_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Check for empty processed texts\u001b[39;00m\n\u001b[32m     42\u001b[39m empty_processed_texts = clean_df[clean_df[\u001b[33m'\u001b[39m\u001b[33mprocessed_text\u001b[39m\u001b[33m'\u001b[39m].str.strip() == \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM4\\amharic-ecommerce-data-extractor\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM4\\amharic-ecommerce-data-extractor\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM4\\amharic-ecommerce-data-extractor\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM4\\amharic-ecommerce-data-extractor\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM4\\amharic-ecommerce-data-extractor\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mpreprocess_amharic_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     25\u001b[39m text = \u001b[38;5;28mstr\u001b[39m(text).lower() \u001b[38;5;66;03m# Convert to string and lowercase\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Remove URLs (common in Telegram posts)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m text = \u001b[43mre\u001b[49m.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttp\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mS+|www\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mS+|https\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mS+\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, text, flags=re.MULTILINE)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Remove Telegram-specific noise (e.g., @mentions, #hashtags if not relevant to product, channel mentions)\u001b[39;00m\n\u001b[32m     29\u001b[39m text = re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m@\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+|#\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+|t.me/\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mS+\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, text)\n",
      "\u001b[31mNameError\u001b[39m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "                                 \n",
    "# --- Step 3: Preprocessing and Structuring the Data ---\n",
    "print(\"\\n--- Initiating Data Preprocessing and Structuring ---\")\n",
    "\n",
    "if raw_df.empty:\n",
    "    print(\"Raw DataFrame is empty. Skipping preprocessing.\")\n",
    "    clean_df = pd.DataFrame(columns=[\n",
    "        'message_id', 'channel_id', 'channel_name', 'date',\n",
    "        'text_content', 'has_media', 'media_type', 'media_file_name',\n",
    "        'views', 'processed_text' # New column for cleaned Amharic text\n",
    "    ])\n",
    "else:\n",
    "    clean_df = raw_df.copy()\n",
    "\n",
    "    # Handle missing text content: Fill None with empty string for NLP\n",
    "    clean_df['text_content'] = clean_df['text_content'].fillna('')\n",
    "\n",
    "    # Convert date to datetime object and extract date only\n",
    "    clean_df['date'] = pd.to_datetime(clean_df['date']).dt.date\n",
    "\n",
    "    # --- Amharic-specific Text Preprocessing (Basic) ---\n",
    "    # For more advanced Amharic NLP, you'd integrate libraries like Ethiopic, AmharicNLP, or custom tokenizers/normalizers.\n",
    "    # For now, we'll do a general cleaning.\n",
    "\n",
    "    def preprocess_amharic_text(text):\n",
    "        text = str(text).lower() # Convert to string and lowercase\n",
    "        # Remove URLs (common in Telegram posts)\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        # Remove Telegram-specific noise (e.g., @mentions, #hashtags if not relevant to product, channel mentions)\n",
    "        text = re.sub(r'@\\w+|#\\w+|t.me/\\S+', '', text)\n",
    "        # Remove non-Amharic characters (excluding basic punctuation if useful, but simpler to remove all non-alphanumeric)\n",
    "        # This regex keeps Amharic characters, spaces, and basic numbers/punctuation (which might be relevant for price/location initially)\n",
    "        # However, for pure text processing, you might only keep Amharic characters.\n",
    "        # \\u1200-\\u137F covers the main Ethiopic script range.\n",
    "        text = re.sub(r'[^\\u1200-\\u137F\\s\\d.,:;!?]+', '', text) # Keep Amharic letters, numbers, basic punctuation, spaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip() # Remove extra spaces\n",
    "        return text\n",
    "\n",
    "    print(\"\\nApplying Amharic-specific text preprocessing...\")\n",
    "    clean_df['processed_text'] = clean_df['text_content'].apply(preprocess_amharic_text)\n",
    "\n",
    "    # Check for empty processed texts\n",
    "    empty_processed_texts = clean_df[clean_df['processed_text'].str.strip() == '']\n",
    "    if not empty_processed_texts.empty:\n",
    "        print(f\"WARNING: {len(empty_processed_texts)} messages became empty after Amharic text preprocessing.\")\n",
    "        # Decide whether to drop these or keep them for their metadata/media. For now, we keep them.\n",
    "\n",
    "    # Drop messages with no meaningful text and no media, as they won't contribute to NER\n",
    "    initial_count = len(clean_df)\n",
    "    clean_df = clean_df[~((clean_df['processed_text'] == '') & (clean_df['has_media'] == False))]\n",
    "    if len(clean_df) < initial_count:\n",
    "        print(f\"Dropped {initial_count - len(clean_df)} rows that had no text content and no media.\")\n",
    "\n",
    "    # Select and reorder columns for the final cleaned CSV\n",
    "    clean_df = clean_df[[\n",
    "        'message_id', 'channel_id', 'channel_name', 'date',\n",
    "        'text_content', 'processed_text', 'has_media', 'media_type', 'media_file_name', 'views'\n",
    "    ]]\n",
    "\n",
    "    print(f\"\\n--- Preprocessing Summary ---\")\n",
    "    print(f\"Total clean messages after preprocessing: {len(clean_df)}\")\n",
    "    print(\"Clean DataFrame Info:\")\n",
    "    clean_df.info()\n",
    "    print(\"\\nFirst 5 rows of Clean Messages:\")\n",
    "    print(clean_df.head())\n",
    "\n",
    "    print(f\"Saving clean messages to: {CLEAN_MESSAGES_CSV}\")\n",
    "    clean_df.to_csv(CLEAN_MESSAGES_CSV, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"\\n--- Task 1: Data Ingestion and Preprocessing Complete ---\")\n",
    "print(\"Remember to commit your work to your 'task-1' branch!\")\n",
    "                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717b9d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
