{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75406c08",
   "metadata": {},
   "source": [
    "# Import independecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7b736e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/01_data_ingestion_preprocessing.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from telethon import TelegramClient, events\n",
    "from telethon.tl.types import MessageMediaPhoto, DocumentAttributeFilename\n",
    "from telethon.tl.functions.channels import GetParticipantsRequest\n",
    "from telethon.tl.types import ChannelParticipantsSearch\n",
    "from tqdm.notebook import tqdm\n",
    "import re \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda532a",
   "metadata": {},
   "source": [
    "# Project root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56614a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM4\\amharic-ecommerce-data-extractor' to sys.path for module imports.\n",
      "Project configuration loaded. Data directories created/verified.\n",
      "Telegram Channels configured: ['@nevacomputer', '@marakibrand', '@Fashiontera', '@Shewabrand', '@ethio_brand_collection']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Project Setup: Ensure src module is discoverable ---\n",
    "# This block is crucial for importing from src.config\n",
    "def find_project_root(current_path):\n",
    "    path = current_path\n",
    "    while path != os.path.dirname(path):\n",
    "        if (os.path.isdir(os.path.join(path, 'src')) and\n",
    "            os.path.isdir(os.path.join(path, 'data')) and\n",
    "            os.path.isdir(os.path.join(path, 'notebooks'))):\n",
    "            return path\n",
    "        path = os.path.dirname(path)\n",
    "    return current_path\n",
    "\n",
    "current_working_dir = os.getcwd()\n",
    "project_root = find_project_root(current_working_dir)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Added '{project_root}' to sys.path for module imports.\")\n",
    "else:\n",
    "    print(f\"'{project_root}' already in sys.path.\")\n",
    "\n",
    "# Import configuration variables\n",
    "from src.config import (\n",
    "    TELEGRAM_API_ID, TELEGRAM_API_HASH, TELEGRAM_CHANNELS,\n",
    "    RAW_MESSAGES_JSON, CLEAN_MESSAGES_CSV, IMAGE_DOWNLOAD_DIR,\n",
    "    RAW_DATA_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17927b57",
   "metadata": {},
   "source": [
    "# Data Ingestiona and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6edfdf4",
   "metadata": {},
   "source": [
    "*Telegram API client setup*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f020f2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-Data Ingestion and Preprocessing ---\n",
      "\n",
      "--- Initiating Telegram Channel Scraping ---\n",
      "\n",
      "--- Connecting to Telegram... ---\n",
      "Connected and authenticated with Telegram successfully.\n",
      "\n",
      "Processing channel: @nevacomputer\n",
      "Resolved channel: NEVA COMPUTERÂ® (ID: 1195361398)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9b6dce60464f8b9b95c96e3c1da5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping NEVA COMPUTERÂ®...:   0%|          | 0/500 [00:00<?, ?msg/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing channel: @marakibrand\n",
      "Resolved channel: áˆ›áˆ«áŠª áƒªÐ¯ï¾‘Å‹ã®â„¢ (ID: 1320403852)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f43c4161e4641bf9af37f5b00697d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping áˆ›áˆ«áŠª áƒªÐ¯ï¾‘Å‹ã®â„¢...:   0%|          | 0/500 [00:00<?, ?msg/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Server closed the connection: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Server closed the connection: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Attempt 1 at connecting failed: OSError: [Errno 10065] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 2 at connecting failed: OSError: [Errno 10065] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 3 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 4 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 5 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 6 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 1 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 2 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 3 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 4 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 5 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 6 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing channel: @Fashiontera\n",
      "Resolved channel: Fashion tera (ID: 1175527648)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0579b3f7d04b7595c2dbda94e65227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping Fashion tera...:   0%|          | 0/500 [00:00<?, ?msg/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Server closed the connection: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Attempt 1 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Server closed the connection: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Attempt 1 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 2 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 3 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 4 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 5 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 6 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Server closed the connection: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Attempt 1 at connecting failed: ConnectionAbortedError: [Errno 10053] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 2 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 3 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 4 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 5 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 6 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Server closed the connection: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Server closed the connection: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Attempt 1 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 2 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 3 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 4 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 5 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n",
      "Attempt 6 at connecting failed: OSError: [Errno 10051] Connect call failed ('149.154.167.91', 443)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing channel: @Shewabrand\n",
      "Resolved channel: Shewa Brand (ID: 1237900032)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11eacc62996c4c0ebd82653201caab25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping Shewa Brand...:   0%|          | 0/500 [00:00<?, ?msg/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing channel: @ethio_brand_collection\n",
      "Resolved channel: EthioBrandÂ® (ID: 1149977975)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01927e8a57d42e984d8628420280e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping EthioBrandÂ®...:   0%|          | 0/500 [00:00<?, ?msg/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Raw Message Collection Summary ---\n",
      "Total raw messages collected: 2500\n",
      "Raw DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2500 entries, 0 to 2499\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   message_id       2500 non-null   int64  \n",
      " 1   channel_id       2500 non-null   int64  \n",
      " 2   channel_name     2500 non-null   object \n",
      " 3   sender_id        2500 non-null   int64  \n",
      " 4   date             2500 non-null   object \n",
      " 5   text_content     2498 non-null   object \n",
      " 6   has_media        2500 non-null   bool   \n",
      " 7   media_type       2482 non-null   object \n",
      " 8   media_file_name  2482 non-null   object \n",
      " 9   media_file_path  2482 non-null   object \n",
      " 10  views            2498 non-null   float64\n",
      "dtypes: bool(1), float64(1), int64(3), object(6)\n",
      "memory usage: 197.9+ KB\n",
      "\n",
      "First 5 rows of Raw Messages:\n",
      "   message_id  channel_id    channel_name      sender_id  \\\n",
      "0        8779  1195361398  NEVA COMPUTERÂ® -1001195361398   \n",
      "1        8778  1195361398  NEVA COMPUTERÂ® -1001195361398   \n",
      "2        8777  1195361398  NEVA COMPUTERÂ® -1001195361398   \n",
      "3        8776  1195361398  NEVA COMPUTERÂ® -1001195361398   \n",
      "4        8775  1195361398  NEVA COMPUTERÂ® -1001195361398   \n",
      "\n",
      "                        date  \\\n",
      "0  2025-06-21T06:32:27+00:00   \n",
      "1  2025-06-21T06:32:27+00:00   \n",
      "2  2025-06-21T06:32:27+00:00   \n",
      "3  2025-06-21T06:32:27+00:00   \n",
      "4  2025-06-11T13:56:52+00:00   \n",
      "\n",
      "                                        text_content  has_media media_type  \\\n",
      "0  ðŸ’» Dell Precision Workstation\\n\\nA powerful per...       True      photo   \n",
      "1                                                          True      photo   \n",
      "2                                                          True      photo   \n",
      "3                                                          True      photo   \n",
      "4                                                          True      photo   \n",
      "\n",
      "                         media_file_name  \\\n",
      "0  channel_1195361398_msg_8779_photo.jpg   \n",
      "1  channel_1195361398_msg_8778_photo.jpg   \n",
      "2  channel_1195361398_msg_8777_photo.jpg   \n",
      "3  channel_1195361398_msg_8776_photo.jpg   \n",
      "4  channel_1195361398_msg_8775_photo.jpg   \n",
      "\n",
      "                                     media_file_path   views  \n",
      "0  c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM4\\amh...   782.0  \n",
      "1  c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM4\\amh...   780.0  \n",
      "2  c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM4\\amh...   780.0  \n",
      "3  c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM4\\amh...   675.0  \n",
      "4  c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM4\\amh...  1751.0  \n",
      "Saving raw messages to: c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM4\\amharic-ecommerce-data-extractor\\data\\raw\\raw_telegram_messages.json\n",
      "Raw messages saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n-Data Ingestion and Preprocessing ---\")\n",
    "\n",
    "# --- Step 1: Telegram API Client Setup ---\n",
    "# A session name is used to save login information so you don't have to log in every time\n",
    "session_name = 'telegram_scraper_session'\n",
    "client = TelegramClient(session_name, TELEGRAM_API_ID, TELEGRAM_API_HASH)\n",
    "\n",
    "async def connect_and_authenticate():\n",
    "    \"\"\"Connects to Telegram and authenticates the user.\"\"\"\n",
    "    print(\"\\n--- Connecting to Telegram... ---\")\n",
    "    await client.start()\n",
    "    if not await client.is_user_authorized():\n",
    "        print(\"Please log in to your Telegram account.\")\n",
    "        # This will prompt for phone number and code in the console if not already logged in\n",
    "        await client.send_code_request(phone=input('Enter phone number: '))\n",
    "        await client.sign_in(phone=input('Enter phone number: '), code=input('Enter code: '))\n",
    "    print(\"Connected and authenticated with Telegram successfully.\")\n",
    "\n",
    "# --- Step 2: Data Ingestion System (Scraping Messages) ---\n",
    "async def scrape_telegram_channels():\n",
    "    \"\"\"Scrapes messages (text and media) from configured Telegram channels.\"\"\"\n",
    "    all_messages_data = []\n",
    "\n",
    "    print(\"\\n--- Initiating Telegram Channel Scraping ---\")\n",
    "    \n",
    "    await connect_and_authenticate()\n",
    "\n",
    "    for channel_id_or_username in TELEGRAM_CHANNELS:\n",
    "        print(f\"\\nProcessing channel: {channel_id_or_username}\")\n",
    "        try:\n",
    "            entity = await client.get_entity(channel_id_or_username)\n",
    "            print(f\"Resolved channel: {entity.title} (ID: {entity.id})\")\n",
    "\n",
    "            # Fetch messages into a list first, then iterate with tqdm\n",
    "            # This resolves the 'async for requires an object with __aiter__' error\n",
    "            # as tqdm.notebook.tqdm expects a synchronous iterable.\n",
    "            # You can adjust the limit for number of messages to fetch per channel.\n",
    "            # For very large channels, consider fetching in smaller chunks or over a specific date range.\n",
    "            messages_fetched = await client.get_messages(entity, limit=500) \n",
    "\n",
    "            pbar_desc = f\"Scraping {entity.title[:20]}...\" # Truncate for display\n",
    "            for message in tqdm(messages_fetched, desc=pbar_desc, unit=\"msg\"):\n",
    "                msg_data = {\n",
    "                    'message_id': message.id,\n",
    "                    'channel_id': entity.id,\n",
    "                    'channel_name': entity.title,\n",
    "                    'sender_id': message.sender_id,\n",
    "                    'date': message.date.isoformat(),\n",
    "                    'text_content': message.message,\n",
    "                    'has_media': False,\n",
    "                    'media_type': None,\n",
    "                    'media_file_name': None,\n",
    "                    'media_file_path': None,\n",
    "                    'views': message.views # Engagement metric\n",
    "                }\n",
    "\n",
    "                if message.media:\n",
    "                    msg_data['has_media'] = True\n",
    "                    if isinstance(message.media, MessageMediaPhoto):\n",
    "                        msg_data['media_type'] = 'photo'\n",
    "                        # Download photo\n",
    "                        photo_filename = f\"channel_{entity.id}_msg_{message.id}_photo.jpg\"\n",
    "                        photo_filepath = os.path.join(IMAGE_DOWNLOAD_DIR, photo_filename)\n",
    "                        try:\n",
    "                            await client.download_media(message.media, file=photo_filepath)\n",
    "                            msg_data['media_file_name'] = photo_filename\n",
    "                            msg_data['media_file_path'] = photo_filepath\n",
    "                        except Exception as e:\n",
    "                            print(f\"  Warning: Could not download photo for message {message.id} in {entity.title}: {e}\")\n",
    "                            msg_data['media_file_name'] = 'download_failed'\n",
    "                    elif message.document: # Handle other documents like files\n",
    "                        for attr in message.document.attributes:\n",
    "                            if isinstance(attr, DocumentAttributeFilename):\n",
    "                                msg_data['media_type'] = 'document'\n",
    "                                doc_filename = f\"channel_{entity.id}_msg_{message.id}_{attr.file_name}\"\n",
    "                                doc_filepath = os.path.join(IMAGE_DOWNLOAD_DIR, doc_filename)\n",
    "                                try:\n",
    "                                    await client.download_media(message.media, file=doc_filepath)\n",
    "                                    msg_data['media_file_name'] = doc_filename\n",
    "                                    msg_data['media_file_path'] = doc_filepath\n",
    "                                except Exception as e:\n",
    "                                    print(f\"  Warning: Could not download document for message {message.id} in {entity.title}: {e}\")\n",
    "                                    msg_data['media_file_name'] = 'download_failed'\n",
    "                                break\n",
    "                all_messages_data.append(msg_data)\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"ERROR: Could not find channel/entity '{channel_id_or_username}'. Please check its exact username or ID. Error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: An unexpected error occurred while processing '{channel_id_or_username}': {e}\")\n",
    "    \n",
    "    await client.disconnect()\n",
    "    return all_messages_data\n",
    "\n",
    "# Run the scraping process\n",
    "# Use asyncio.run() to run the async function\n",
    "messages_list = await scrape_telegram_channels() # In a Jupyter notebook, 'await' works directly at top-level\n",
    "\n",
    "if not messages_list:\n",
    "    print(\"\\nCRITICAL WARNING: No messages were collected from any channel. Please check channel configurations and API keys.\")\n",
    "    raw_df = pd.DataFrame()\n",
    "else:\n",
    "    print(f\"\\n--- Raw Message Collection Summary ---\")\n",
    "    raw_df = pd.DataFrame(messages_list)\n",
    "    print(f\"Total raw messages collected: {len(raw_df)}\")\n",
    "    print(\"Raw DataFrame Info:\")\n",
    "    raw_df.info()\n",
    "    print(\"\\nFirst 5 rows of Raw Messages:\")\n",
    "    print(raw_df.head())\n",
    "    print(f\"Saving raw messages to: {RAW_MESSAGES_JSON}\")\n",
    "    # Save as JSON as it's easier to store complex dicts, especially if media info gets complicated\n",
    "    with open(RAW_MESSAGES_JSON, 'w', encoding='utf-8') as f:\n",
    "        json.dump(messages_list, f, ensure_ascii=False, indent=4)\n",
    "    print(\"Raw messages saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b077306",
   "metadata": {},
   "source": [
    "# Preprocessing and Structuring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd162f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Initiating Data Preprocessing and Structuring ---\n",
      "\n",
      "Applying Amharic-specific text preprocessing...\n",
      "WARNING: 1094 messages became empty after Amharic text preprocessing.\n",
      "Dropped 3 rows that had no text content and no media.\n",
      "\n",
      "--- Preprocessing Summary ---\n",
      "Total clean messages after preprocessing: 2497\n",
      "Clean DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2497 entries, 0 to 2499\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   message_id       2497 non-null   int64  \n",
      " 1   channel_id       2497 non-null   int64  \n",
      " 2   channel_name     2497 non-null   object \n",
      " 3   date             2497 non-null   object \n",
      " 4   text_content     2497 non-null   object \n",
      " 5   processed_text   2497 non-null   object \n",
      " 6   has_media        2497 non-null   bool   \n",
      " 7   media_type       2482 non-null   object \n",
      " 8   media_file_name  2482 non-null   object \n",
      " 9   views            2497 non-null   float64\n",
      "dtypes: bool(1), float64(1), int64(2), object(6)\n",
      "memory usage: 197.5+ KB\n",
      "\n",
      "First 5 rows of Clean Messages:\n",
      "   message_id  channel_id    channel_name        date  \\\n",
      "0        8779  1195361398  NEVA COMPUTERÂ®  2025-06-21   \n",
      "1        8778  1195361398  NEVA COMPUTERÂ®  2025-06-21   \n",
      "2        8777  1195361398  NEVA COMPUTERÂ®  2025-06-21   \n",
      "3        8776  1195361398  NEVA COMPUTERÂ®  2025-06-21   \n",
      "4        8775  1195361398  NEVA COMPUTERÂ®  2025-06-11   \n",
      "\n",
      "                                        text_content  \\\n",
      "0  ðŸ’» Dell Precision Workstation\\n\\nA powerful per...   \n",
      "1                                                      \n",
      "2                                                      \n",
      "3                                                      \n",
      "4                                                      \n",
      "\n",
      "                                      processed_text  has_media media_type  \\\n",
      "0  , , , . 7 9 8 16 4.7 , , . 32 4 , , . 1 , , . ...       True      photo   \n",
      "1                                                          True      photo   \n",
      "2                                                          True      photo   \n",
      "3                                                          True      photo   \n",
      "4                                                          True      photo   \n",
      "\n",
      "                         media_file_name   views  \n",
      "0  channel_1195361398_msg_8779_photo.jpg   707.0  \n",
      "1  channel_1195361398_msg_8778_photo.jpg   707.0  \n",
      "2  channel_1195361398_msg_8777_photo.jpg   707.0  \n",
      "3  channel_1195361398_msg_8776_photo.jpg   613.0  \n",
      "4  channel_1195361398_msg_8775_photo.jpg  1694.0  \n",
      "Saving clean messages to: c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM4\\amharic-ecommerce-data-extractor\\data\\processed\\clean_telegram_messages.csv\n",
      "\n",
      " Data Ingestion and Preprocessing Complete ---\n"
     ]
    }
   ],
   "source": [
    "                                 \n",
    "# --- Step 3: Preprocessing and Structuring the Data ---\n",
    "print(\"\\n--- Initiating Data Preprocessing and Structuring ---\")\n",
    "\n",
    "if raw_df.empty:\n",
    "    print(\"Raw DataFrame is empty. Skipping preprocessing.\")\n",
    "    clean_df = pd.DataFrame(columns=[\n",
    "        'message_id', 'channel_id', 'channel_name', 'date',\n",
    "        'text_content', 'has_media', 'media_type', 'media_file_name',\n",
    "        'views', 'processed_text' # New column for cleaned Amharic text\n",
    "    ])\n",
    "else:\n",
    "    clean_df = raw_df.copy()\n",
    "\n",
    "    # Handle missing text content: Fill None with empty string for NLP\n",
    "    clean_df['text_content'] = clean_df['text_content'].fillna('')\n",
    "\n",
    "    # Convert date to datetime object and extract date only\n",
    "    clean_df['date'] = pd.to_datetime(clean_df['date']).dt.date\n",
    "\n",
    "    # --- Amharic-specific Text Preprocessing (Basic) ---\n",
    "    # For more advanced Amharic NLP, you'd integrate libraries like Ethiopic, AmharicNLP, or custom tokenizers/normalizers.\n",
    "    # For now, we'll do a general cleaning.\n",
    "\n",
    "    def preprocess_amharic_text(text):\n",
    "        text = str(text).lower() # Convert to string and lowercase\n",
    "        # Remove URLs (common in Telegram posts)\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        # Remove Telegram-specific noise (e.g., @mentions, #hashtags if not relevant to product, channel mentions)\n",
    "        text = re.sub(r'@\\w+|#\\w+|t.me/\\S+', '', text)\n",
    "        # Remove non-Amharic characters (excluding basic punctuation if useful, but simpler to remove all non-alphanumeric)\n",
    "        # This regex keeps Amharic characters, spaces, and basic numbers/punctuation (which might be relevant for price/location initially)\n",
    "        # However, for pure text processing, you might only keep Amharic characters.\n",
    "        # \\u1200-\\u137F covers the main Ethiopic script range.\n",
    "        text = re.sub(r'[^\\u1200-\\u137F\\s\\d.,:;!?]+', '', text) # Keep Amharic letters, numbers, basic punctuation, spaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip() # Remove extra spaces\n",
    "        return text\n",
    "\n",
    "    print(\"\\nApplying Amharic-specific text preprocessing...\")\n",
    "    clean_df['processed_text'] = clean_df['text_content'].apply(preprocess_amharic_text)\n",
    "\n",
    "    # Check for empty processed texts\n",
    "    empty_processed_texts = clean_df[clean_df['processed_text'].str.strip() == '']\n",
    "    if not empty_processed_texts.empty:\n",
    "        print(f\"WARNING: {len(empty_processed_texts)} messages became empty after Amharic text preprocessing.\")\n",
    "        # Decide whether to drop these or keep them for their metadata/media. For now, we keep them.\n",
    "\n",
    "    # Drop messages with no meaningful text and no media, as they won't contribute to NER\n",
    "    initial_count = len(clean_df)\n",
    "    clean_df = clean_df[~((clean_df['processed_text'] == '') & (clean_df['has_media'] == False))]\n",
    "    if len(clean_df) < initial_count:\n",
    "        print(f\"Dropped {initial_count - len(clean_df)} rows that had no text content and no media.\")\n",
    "\n",
    "    # Select and reorder columns for the final cleaned CSV\n",
    "    clean_df = clean_df[[\n",
    "        'message_id', 'channel_id', 'channel_name', 'date',\n",
    "        'text_content', 'processed_text', 'has_media', 'media_type', 'media_file_name', 'views'\n",
    "    ]]\n",
    "\n",
    "    print(f\"\\n--- Preprocessing Summary ---\")\n",
    "    print(f\"Total clean messages after preprocessing: {len(clean_df)}\")\n",
    "    print(\"Clean DataFrame Info:\")\n",
    "    clean_df.info()\n",
    "    print(\"\\nFirst 5 rows of Clean Messages:\")\n",
    "    print(clean_df.head())\n",
    "\n",
    "    print(f\"Saving clean messages to: {CLEAN_MESSAGES_CSV}\")\n",
    "    clean_df.to_csv(CLEAN_MESSAGES_CSV, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"\\n Data Ingestion and Preprocessing Complete ---\")\n",
    "\n",
    "                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717b9d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
